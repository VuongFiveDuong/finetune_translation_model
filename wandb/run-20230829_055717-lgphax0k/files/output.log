  0% 0/438 [00:00<?, ?it/s]You're using a MBartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.





















 23% 100/438 [00:43<02:22,  2.37it/s]











 85% 11/13 [00:24<00:04,  2.24s/it]




















 46% 200/438 [01:52<01:30,  2.63it/s]











 92% 12/13 [00:26<00:02,  2.01s/it]





















 68% 300/438 [03:03<00:55,  2.48it/s]









 92% 12/13 [00:24<00:02,  2.16s/it]





















 91% 400/438 [04:11<00:15,  2.38it/s]










 85% 11/13 [00:22<00:03,  1.99s/it]







 99% 434/438 [04:52<00:01,  2.47it/s]
{'train_runtime': 298.2406, 'train_samples_per_second': 11.735, 'train_steps_per_second': 1.469, 'train_loss': 1.026089672628603, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     1.0261
  train_runtime            = 0:04:58.24
  train_samples            =       3500
  train_samples_per_second =     11.735

100% 438/438 [04:54<00:00,  1.49it/s]