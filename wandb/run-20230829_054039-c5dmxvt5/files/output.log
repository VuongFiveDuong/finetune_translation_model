
  0% 0/875 [00:00<?, ?it/s]You're using a MBartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

















 11% 100/875 [00:38<04:29,  2.87it/s]
























 96% 24/25 [09:43<00:16, 16.69s/it]

















 23% 200/875 [11:29<03:58,  2.83it/s]




Traceback (most recent call last):
  File "/usr/lib/python3.10/linecache.py", line 46, in getlines
    return updatecache(filename, module_globals)
  File "/usr/lib/python3.10/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/usr/lib/python3.10/tokenize.py", line 394, in open
    buffer = _builtin_open(filename, 'rb')
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "/content/gdrive/MyDrive/VinUni/Finetune-translation-model/finetune-translation-model/finetuning_vinai.py", line 148, in <module>
    train_result = trainer.train()
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 1555, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 2256, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py", line 159, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 2972, in evaluate
    output = eval_loop(
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 3161, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py", line 282, in prediction_step
    generated_tokens = self.model.generate(**inputs, **gen_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 1675, in generate
    return self.beam_search(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3065, in beam_search
    beam_outputs = beam_scorer.process(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/beam_search.py", line 296, in process
    if beam_idx < self.group_size:
KeyboardInterrupt